# Regulatory and Compliance Considerations in AI Security

Views on AI security regulations are as diverse as the cultures of the world, all of whom are affected by the rise of AI in the daily life of so many different people. New laws and regulations took effect in 2024-2025, while others walked back previously established policies. This is particularly of note with the European Union's AI Act nearing implementation, and the present administration in the United States reversing course on federal AI policy that had been established by the previous administration. Western countries weren't the only ones who saw changes, either. Diverse Asia-Pacific nations implemented equally diverse approaches to AI security, ranging from China's strict controls to Japan's voluntary frameworks.

Furthermore, organizations deploying Large Language Models (LLMs) now face intricate security requirements that span more than 50 jurisdictions, with penalties reaching as high as €35 million or 7% of global revenue for violations. This chapter aims to provide a comprehensive guide to organizations looking to navigate current global regulations, as well as helping prepare for pending legislation currently in the works and set to take effect within the next few years. The chapter also offers suggestions on implementing technical compliance measures in major markets, while providing insights on how to manage escalating costs surrounding AI, which are projected to reach $5.2 billion within the next 5 years.

The diversity in regulatory approaches creates both exciting opportunities and significant challenges for organizations working with AI, ostensibly due to the varying compliance requirements for global companies. While the EU has chosen to implement comprehensive security-by-design requirements that will take effect by August 2026, the Trump administration's January 2025 executive order completely dismantled federal AI safety requirements. This sudden reversal by the Trump administration created what many experts describe as a patchwork of state regulations. North of the United States, Canada's proposed AIDA (Artificial Intelligence and Data Act) includes the possibility of criminal liability for reckless AI deployment. On the other side of the globe, China requires pre-deployment algorithm registration along with strict content controls, while Singapore offers helpful voluntary AI Verify frameworks. Understanding these varied approaches becomes absolutely critical for organizations that operate across international borders.

## How current regulations are reshaping AI security around the world

When examining the current state of AI regulations, it's impossible to ignore the European Union's groundbreaking approach. The EU AI Act officially took effect on August 1, 2024, representing what many legal experts consider the world's most comprehensive AI regulatory framework. The legislation takes a risk-based approach that's quite different from traditional tech regulations. **Article 15 of the Act mandates that high-risk AI systems achieve "appropriate levels of accuracy, robustness, and cybersecurity"** throughout their entire operational lifecycle, which includes specific protections against data poisoning, model evasion, and adversarial attacks that have become increasingly common in recent years.

What makes the EU approach particularly noteworthy is how it handles large-scale AI systems. General-purpose AI models that exceed 10^25 FLOPs (floating-point operations) face what the legislation calls "enhanced obligations." These requirements include mandatory red-teaming exercises and comprehensive systemic risk assessments that must be completed by August 2025. For organizations developing or planning to deploy these massive models, this creates a significant and expensive compliance burden that requires careful planning and substantial resource investments.

Across the Atlantic, the United States took a dramatically different path that caught many industry observers and experts by surprise. President Trump signed Executive Order 14148 on January 20, 2025, which completely revoked the Biden administration's AI safety requirements put in place a little over a year earlier. This controversial reversal eliminated mandatory reporting requirements for dual-use foundation models and dissolved the 90-day cybersecurity protection requirements that had previously applied to companies developing models with more than 10^26 compute operations.

This sudden federal policy reversal introduced an interesting dynamic at the state level, where California has taken the lead among U.S. states where AI policy is concerned. The Golden State responded to the federal vacuum by implementing **18 new AI laws that took effect in January 2025**. These laws include requirements for comprehensive AI system documentation, explicit digital replica consent procedures, and strict prohibitions on political deepfakes. However, California's controversial SB 1047, which would have required companies to implement a "kill switch" for large AI models, was ultimately vetoed by Governor Gavin Newsom, highlighting the ongoing tension between safety concerns and innovation priorities.

Moving across the Pacific, China has implemented what many observers describe as the most rigorous approach to AI regulation anywhere in the world. The country's Interim Measures for the Administration of Generative AI Services became effective on August 15, 2023, and they establish a comprehensive pre-approval system. **All AI services with "public opinion attributes or social mobilization capabilities" must file with the Cyberspace Administration of China** before they can be deployed to Chinese users. As of December 2024, this registry included 302 different AI systems, providing critical insights into just how thoroughly the Chinese government monitors and controls AI deployments within its borders.

The Chinese framework goes beyond simple registration requirements. It mandates comprehensive content labeling for AI-generated materials, strictly prohibits outputs that could endanger what the government defines as national security, and holds AI providers legally liable for generated content that fails to uphold "Core Socialist Values." This approach represents a stark contrast to the more hands-off regulatory philosophies seen in other parts of the world.

In the western world, the United Kingdom chose a different path, implementing what government officials call a "principles-based framework that prioritizes innovation over rigid regulatory structures." Rather than passing new legislation, the UK relies on five cross-sector principles implemented via existing regulatory bodies: safety and robustness, transparency, fairness, accountability, and contestability. The Financial Conduct Authority reported that **75% of UK financial firms were already using AI by 2024**, with cybersecurity consistently ranked as the highest systemic risk concern. The Bank of England maintains that existing model risk management principles sufficiently govern AI applications without requiring completely new regulatory frameworks.

Canada is developing its own comprehensive approach through the Artificial Intelligence and Data Act (AIDA), introduced as part of Bill C-27 in June 2022. This legislation is working its way through parliamentary approval, with implementation expected no earlier than 2027. The Canadian framework targets "high-impact AI systems" that could significantly affect health, safety, and individual rights, requiring **comprehensive risk assessments, robust governance mechanisms, and clear accountability frameworks**. Canada's approach stands out for its severe penalties: fines reaching CAD $25 million or 5% of global revenue, plus potential criminal liability with prison sentences up to 5 years for individuals involved in reckless AI deployment causing serious harm.

The Asia-Pacific region demonstrates remarkable diversity in regulatory approaches. Japan pursues "agile governance" through voluntary guidelines, positioning itself as the "most AI-friendly nation" with minimal binding requirements. South Korea passed its Framework Act on Artificial Intelligence Development in December 2024, making it the second entity after the EU with comprehensive AI legislation taking effect in January 2026. Singapore emphasizes technical testing through its Model AI Governance Framework and AI Verify program, providing organizations with **11 specific AI ethics principles along with corresponding technical tests**. Australia proposed 10 mandatory guardrails in September 2024, covering accountability, risk management, and transparency for high-risk systems.

## Understanding what pending legislation means for the future

The United States Congress faced significant challenges in passing comprehensive AI legislation throughout the 118th Congress, despite introducing more than 150 AI-related bills. Most legislative efforts stalled due to political gridlock, with only limited defense-related provisions becoming law. This congressional inaction created uncertainty for organizations planning compliance strategies.

The 119th Congress now faces political tension around AI regulation, particularly regarding the 10th Amendment. House Republicans advanced a controversial **10-year moratorium on state AI law enforcement** through budget reconciliation, arguing that state regulation patchworks create unworkable business environments. However, more than 40 state attorneys general have opposed this proposal, citing constitutional concerns about states' rights to protect citizens. Federal bills receiving attention include the AI Development Practices Act, requiring NIST security evaluations, and the AI Research, Innovation, and Accountability Act, proposing federal oversight for high-impact systems.

Across the Atlantic, the European Commission continues developing technical standards for the AI Act throughout 2025-2026, though significant delays have emerged. Harmonized standards from CEN/CENELEC face **8-month delays, with completion now expected at the end of 2025** rather than the original April deadline. The General-Purpose AI Code of Practice third draft, published in March 2025, provides voluntary compliance mechanisms while mandatory requirements develop through formal legislative processes. All EU member states must establish AI regulatory sandboxes by August 2026, offering free participation for organizations testing innovative AI applications under relaxed regulatory conditions.

In South America, Brazil leads Latin American AI regulation through Bill 2,338/2023, which received Senate approval in December 2024 and awaits House consideration. The Brazilian framework establishes **risk-based classifications closely mirroring the EU AI Act**, requiring algorithmic impact assessments for high-risk systems and prohibiting AI applications incompatible with human rights. Mexico proposed constitutional amendments in February 2025 granting Congress authority to regulate artificial intelligence, with a 180-day timeline for comprehensive regulations if approved.

China's regulatory expansion continues with three national cybersecurity standards for generative AI taking effect November 1, 2025. **AI content labeling rules become mandatory in September 2025**, requiring non-intrusive but visible identifiers on all AI-generated content. While China develops comprehensive AI law, full enactment appears unlikely before 2026, maintaining focus on expanding algorithmic registry systems and enhancing pre-deployment security assessments.

International coordination accelerates through multiple channels. The G7 Hiroshima Process established AI Principles and Code of Conduct in October 2023, now implemented through an **OECD reporting framework launching February 2025** for voluntary adoption across 47 jurisdictions. The UN AI Advisory Body's "Governing AI for Humanity" report proposes comprehensive international governance frameworks, while UNESCO's AI Ethics Recommendation achieved adoption by 194 member states, emphasizing rights-based development approaches.

## Why Large Language Models face particularly intense scrutiny

LLMs have become the focus of especially intense regulatory attention due to their dual-use potential and systemic risk characteristics. These systems serve beneficial purposes like education and productivity enhancement, but malicious actors can misuse them for disinformation campaigns, cyberattacks, or other harmful activities. This dual nature prompted regulators worldwide to develop specific frameworks for governing LLM development and deployment.

The EU AI Act takes a detailed approach to regulating large language models. The legislation classifies models exceeding **10^25 FLOPs as presenting systemic risk**, triggering obligations including mandatory model evaluation procedures, adversarial testing requirements, risk mitigation planning, and incident reporting to regulatory authorities. Organizations working with these models must maintain detailed technical documentation covering training processes, datasets used, computational resources required, and comprehensive risk assessments. Additionally, they must provide detailed capability and limitation information to downstream providers, creating complex disclosure requirements throughout the AI supply chain.

The revoked US Executive Order 14110 had required companies developing dual-use foundation models to report cybersecurity protections and red-team testing results to federal authorities within 90 days. While the Trump administration eliminated these federal requirements, **California's pending regulations** and sector-specific guidance from agencies like the FDA and financial regulators continue pressuring organizations toward responsible LLM deployment practices. The NIST AI Risk Management Framework remains voluntary at the federal level but sees widespread adoption by organizations demonstrating responsible AI practices.

Global data protection laws significantly impact LLM development and deployment, creating complex compliance challenges for organizations operating across multiple jurisdictions. GDPR requires lawful basis for personal data in training datasets, with the **European Data Protection Board confirming that AI models may contain personal data** requiring ongoing compliance throughout operational lifecycles. Cross-border data transfers for model training face restrictions under adequacy decisions and Standard Contractual Clauses. China prohibits collecting unnecessary personal information for AI training, while comprehensive data governance measures apply throughout AI lifecycles.

Model evaluation and testing mandates vary significantly by jurisdiction. The EU requires standardized evaluations for systemic risk models, China mandates pre-deployment security assessments, and Singapore's AI Verify provides **11 specific AI ethics principles with corresponding technical tests**. Industry consortiums like MLCommons develop benchmarking frameworks, while the Partnership on AI creates safety evaluation protocols voluntarily adopted by major AI developers.

Supply chain security emerged as critical following a **156% increase in malicious packages** targeting AI development repositories throughout 2024. These attacks frequently target Python ecosystems commonly used in AI development, attempting to insert malicious code into widely-used libraries and frameworks. The EU AI Act requires comprehensive supply chain documentation, while US agencies emphasize maintaining detailed software bills of materials for AI systems.

## How technical compliance requirements span the entire development lifecycle

Security by design represents the most fundamental technical requirement across jurisdictions, requiring organizations to build security considerations into AI systems from development's beginning rather than adding them later. The EU AI Act's Article 15 establishes detailed requirements for **protection against data poisoning, model poisoning, adversarial examples, and confidentiality attacks** using technical solutions appropriate to circumstances and risk levels.

NIST's Secure Software Development Framework provides AI-specific best practices that many organizations voluntarily adopt, including input validation procedures, output sanitization mechanisms, model integrity verification systems, and secure deployment pipelines throughout development lifecycles. Organizations adopting these practices early position themselves ahead of evolving regulatory requirements.

Auditing and monitoring requirements have intensified for high-risk AI systems, reflecting regulators' recognition that AI systems can change and drift over time unlike traditional software. The EU mandates comprehensive post-market monitoring plans with **10-year retention requirements for logs and documentation**, real-time performance tracking, and serious incident reporting to supervisory authorities. Technical implementations include immutable tamper-evident logging systems, role-based access controls limiting system configuration modifications, automated anomaly detection capabilities, and forensic tools for investigating security events. For LLM interactions, organizations must implement complete logging of user inputs, system outputs, model decisions, and data access patterns.

Documentation transparency requirements extend beyond traditional software expectations. The EU AI Act's Annex XI specifies **mandatory technical documentation elements** including detailed system descriptions, risk assessments, training procedures, model architectures, and quality management systems. Organizations must maintain this documentation throughout operational lifecycles and provide it to regulatory authorities upon request. Model cards, pioneered by Google and enhanced through NVIDIA's Model Card++ framework, now include dedicated security sections, demographic performance metrics, limitation disclosures, and bias assessment results. Documentation must be machine-readable for automated compliance checking with comprehensive version control.

Testing requirements have escalated dramatically across jurisdictions, reflecting unique AI security challenges. Red-teaming is mandatory for EU systemic risk models and was required for US dual-use foundation models before the executive order reversal. The **MITRE ATLAS framework** provides systematic adversarial testing approaches including prompt injection resistance, data poisoning resilience, and model extraction protection. ISO/IEC 42001 requires lifecycle security testing with intensity based on risk classification, while penetration testing adapts OWASP guidelines for AI-specific vulnerabilities.

Access control and governance frameworks require sophisticated zero-trust architectures with continuous verification. Technical requirements include **role-based or attribute-based access control systems**, multi-factor authentication for administrative access, privileged access workstations for high-privilege operations, and network microsegmentation providing isolation between system components. API security follows OWASP standards while adding AI-specific attack vector considerations, including strong authentication mechanisms, fine-grained authorization controls, comprehensive rate limiting, and extensive input validation across endpoints.

## How industry-specific regulations create additional compliance layers

Financial services organizations face the most mature and complex AI regulatory landscape. The Basel Committee requires banks to establish **appropriate governance, risk management, and control frameworks** for AI/ML systems under BCBS 239 principles, originally designed for data risk management but now covering AI applications. Model risk management under Federal Reserve SR 11-7 extends to AI with enhanced validation and monitoring beyond traditional financial modeling. Banks must demonstrate that AI systems are accurate, reliable, explainable, and fair in decision-making. The European Securities and Markets Authority's May 2024 guidance mandates best-interest obligations, risk controls, transparency measures, and human oversight for AI-driven investment advice under MiFID II.

Healthcare AI confronts stringent requirements across overlapping frameworks, reflecting life-and-death consequences. The FDA's Good Machine Learning Practice outlines **10 core principles** including multi-disciplinary expertise requirements, representative datasets reflecting patient population diversity, independent test sets, and monitoring capabilities detecting performance degradation. Predetermined Change Control Plans enable pre-authorized model updates within risk-based categories, allowing healthcare AI improvements without full regulatory review for each change. HIPAA compliance adds complexity through Business Associate Agreements for AI vendors, strict minimum necessary PHI access controls, detailed audit trails, and enhanced consent procedures for AI-driven care decisions.

Critical infrastructure sectors adapt existing cybersecurity frameworks for AI challenges. NERC CIP standards governing electric grid cybersecurity require **proper categorization of AI systems as BES Cyber Systems**, electronic security perimeters around AI infrastructure, comprehensive patch management, and detailed supply chain risk assessments. The Transportation Security Administration issued emergency cybersecurity amendments mandating critical system protection, annual assessment plans, and vendor management for AI service providers. Telecommunications face FCC equipment security requirements including national security risk assessments for AI-enabled infrastructure.

Government and defense applications operate under the Department of Defense's five ethical AI principles: Responsible, Equitable, Traceable, Reliable, and Governable. The Chief Digital and Artificial Intelligence Office provides **detailed implementation toolkits, governance structures, and verification frameworks**. Classified AI systems face enhanced security clearances for development personnel, isolated development environments, and real-time monitoring capabilities. Export controls under ITAR and EAR restrict AI technology transfers, with deemed export rules applying to foreign national access within the United States.

Additional regulated sectors face evolving requirements as regulators gain AI experience. Education confronts expanded FERPA definitions including **AI-generated insights as educational records** requiring consent procedures, data minimization, and audit monitoring. Legal services must ensure AI competence under ABA Model Rule 1.1, maintain client confidentiality with AI tools, obtain proper consent for AI-assisted work, and provide appropriate supervision. Real estate applications must prevent algorithmic bias in property valuations and lending under Fair Housing Act requirements.

## What enforcement actions reveal about regulatory priorities

The enforcement landscape transformed dramatically over 18 months, with penalties reaching unprecedented levels that captured worldwide organizational attention. The EU AI Act establishes **the world's highest penalties at €35 million or 7% of global revenue** for prohibited AI practices, representing significant escalation from previous technology regulations. The penalty structure includes tiered levels: €15 million or 3% for high-risk system non-compliance, and €7.5 million or 1% for providing false information to authorities. The EU added protections for small and medium enterprises through penalty calculations using whichever amount is less favorable, providing relief for organizations lacking tech giant revenue streams.

US enforcement dispersed across federal and state levels following the Trump administration's executive order reversal. The FTC's Operation AI Comply resulted in **five significant enforcement actions, including a $193,000 penalty against DoNotPay** for unsubstantiated AI legal service claims. IntelliVision Technologies faced consent decrees for false bias-free facial recognition claims, while Rite Aid received a 5-year facial recognition ban for racially biased surveillance violating consumer protection laws. State-level enforcement proved aggressive, with Texas achieving a record $1.4 billion Meta settlement for biometric data violations, demonstrating how state authorities fill federal oversight voids.

Compliance costs skyrocket across sectors, with the **AI compliance monitoring market projected to grow from $1.8 billion to $5.2 billion by 2030** at 19.4% compound annual growth. McKinsey surveys found 13% of organizations hired AI compliance specialists by 2024, while 60% of compliance officers plan significant AI-powered RegTech investments by 2025. Healthcare and financial services face higher premiums due to operational risks and enhanced regulatory scrutiny.

Common compliance failures include inadequate bias testing, false performance claims in marketing, insufficient human oversight of automated decisions, data consent violations in training datasets, and lack of algorithmic transparency in high-stakes applications. The FTC focuses on **algorithmic auditing requirements, truth in AI advertising, children's privacy protection, and discrimination prevention**. The EU AI Office emphasizes general-purpose model compliance, high-risk system certification, and cross-border enforcement coordination.

Litigation trends reveal expanding private rights with approximately 25 copyright cases against AI companies, surging privacy class actions under state biometric laws, and **AI-related securities cases doubling in 2024** addressing "AI washing" claims. Insurance coverage struggles with emerging risks, including AI-specific exclusions, gaps in data poisoning and algorithmic bias coverage, and premiums increasing over 50% annually, driving specialized AI liability policy development.

## Understanding future trends and proactive compliance strategies

The regulatory trajectory for 2025-2026 accelerates with EU AI Act phased implementation serving as a global template. Implementation milestones include **general-purpose AI obligations by August 2025**, high-risk system requirements by August 2026, and extended transitions for embedded AI in regulated products through 2027. Technical standards face delays requiring organizations to proceed with compliance planning using draft standards rather than waiting for final specifications.

Emerging technology regulations address critical framework gaps. Multi-modal AI security requirements tackle **cross-modal attack vectors exploiting interactions between different AI system types**, privacy preservation in federated learning environments, and standardized evaluation frameworks across AI applications. Quantum-safe AI security becomes urgent given 17-34% Q-Day probability by 2034, accelerating crypto-agility requirements and quantum-resistant algorithm migration planning. Edge AI faces resource-constrained security challenges requiring innovative distributed threat detection approaches.

International harmonization accelerates through multiple channels. The G7 Hiroshima Process established **risk-based lifecycle approaches with pre-deployment assessments**, while 47 OECD jurisdictions adopted enhanced principles emphasizing safety, risk management, and information integrity. ISO/IEC standards provide interoperable AI governance foundations, with December 2025 Seoul summit advancing reliability, sustainability, and human rights standards. Mutual recognition frameworks emerge through bilateral agreements and shared certification approaches.

The threat landscape evolves with supply chain attacks increasing 156% in 2024, targeting Python ecosystems prevalent in AI development and compromising widely-used libraries. Advanced persistent threats target **CI/CD pipelines, version control platforms, and documentation tools** used by development teams. Model extraction and intellectual property theft drive enhanced trade secret protections and tighter export controls. Deepfake regulations implement mandatory content labeling, with China requiring compliance by September 2025 and the EU by August 2026.

Regulatory innovation introduces adaptive mechanisms including AI regulatory sandboxes required in all EU member states by August 2026, evidence-based policy development through sandbox outcomes, and **AI-powered compliance monitoring tools** managing complex requirements. Public-private partnerships expand through industry-government collaboration on safety research, shared threat intelligence platforms, and co-developed technical standards addressing rapidly evolving AI capabilities.

## Building comprehensive organizational compliance frameworks

Organizations need immediate action to position themselves for the evolving regulatory landscape. This begins with conducting AI system inventories classifying systems according to EU AI Act risk categories, even for non-European operations, since these classifications become global standards. Implementing mandatory AI literacy programs became EU legal requirements in February 2025, but worldwide organizations should consider similar training ensuring teams understand regulatory requirements and responsible AI practices.

Establishing **robust supply chain security monitoring** proves critical following the 156% increase in attacks targeting AI development repositories. Organizations must implement software dependency monitoring, secure development pipelines, and detailed software bills of materials for all AI systems. Beginning quantum-safe cryptography transition planning should start immediately given approaching Q-Day probabilities and required lead times. Documentation frameworks must support 10-year retention requirements with machine-readable formats enabling automated compliance checking.

Medium-term preparations require developing AI governance frameworks aligned with ISO/IEC 42001 standards, becoming global benchmarks for AI management systems. Organizations should prepare detailed technical documentation for high-risk systems including **comprehensive risk assessments, detailed model architectures, and robust quality management systems**. Implementing red teaming programs using MITRE ATLAS frameworks becomes essential for large-scale AI systems, including regular adversarial testing, prompt injection resistance evaluation, and data poisoning vulnerability assessment. Establishing continuous monitoring infrastructure with automated anomaly detection and forensic capabilities enables quick security incident detection and response while maintaining regulatory reporting records.

Strategic considerations for long-term success include building adaptive compliance capabilities evolving with regulations, since the landscape develops rapidly. Investing in **AI-powered regulatory technology** helps organizations manage increasing complexity across jurisdictions by automating compliance monitoring, documentation generation, and regulatory change tracking. Developing international coordination mechanisms proves essential for global organizations, including establishing regulatory authority relationships, participating in industry associations and standards development, and maintaining flexible architectures accommodating varying requirements.

Risk management priorities focus on implementing comprehensive technical controls against adversarial attacks, establishing thorough testing throughout AI lifecycles, ensuring transparent documentation and explainability measures, and **developing robust incident response procedures** with integrated regulatory notification capabilities. Human oversight remains critical despite automation advances, requiring clear accountability structures defining roles and responsibilities, competent AI governance teams with technical and legal expertise, and well-defined escalation procedures for high-stakes decisions.

Privacy and data protection considerations underpin all AI security efforts, including establishing lawful basis for training data collection and use, ensuring cross-border transfer compliance, implementing comprehensive data governance throughout AI lifecycles, and maintaining breach notification readiness with jurisdiction-specific procedures.

The convergence of comprehensive regulations, evolving technical standards, and changing threat landscapes creates unprecedented AI security compliance complexity. Success requires proactive regulatory engagement, significant investment in robust technical controls, and strong organizational commitment to responsible AI deployment. As the regulatory landscape continues evolving through 2026 and beyond, maintaining flexibility while ensuring fundamental security protections represents the critical balance organizations must strike for sustainable AI innovation within increasingly complex regulatory boundaries.
